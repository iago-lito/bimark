# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
# This model implements a classical, naive sampling from the polytope:
# draw random points in the bounding box, then reject them or not based on the
# positivity of x. This equivalent to Simon Bonner's procedure.
#
# Pro: easy to grasp and easy to implement
# Cons: leads to a *really* high level of rejection, since (as far as I have
# tried), a small polytope may easily fit in less than 1% of its bounding box.
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

# Describe all nodes here:# {{{
var

# First, data nodes: they are fixed # {{{ - - - - - - - - - - - - - - - - - - -
# number of capture occasions
  T,
# number of capture events
  nbCaptureEvents,
# number various types of histories to consider
  LS, LL, LR, LU, LB, LM, L,
# data counts for the various histories
  L.f[LL],
  R.f[LR],
  is.Sf.empty, # special degenerated case to handle
  jagsLS, # virtual LS for jags: equals LS OR 1 if LS=0
  jagsS.f[jagsLS], # virtual S.F for jags: dummy data if LS=0
# the actual histories
  Omega[L,T],
# upper bounds of the hyperbox bounding the polytope (lower bounds are 0)
  box.ceilings[LB],
# nullspace of the A matrix, describing the polytope from which to sample d.
# It also provides a way to rebuild x = B.d + x0
  B[LL + LR + LL*LR, LL * LR],
# }}} - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Second, prior "hyper"parameters: they are also fixed # {{{ - - - - - - - - - -
# beta hyperparameters for capture probabilities on each occasion
  h.P[2],
# dirichlet parameters for capture events probabilities given that a
# capture occured (its first element ('no capture') will be set to zero)
  h.delta[nbCaptureEvents],
# Uniform prior for the polytope
# TODO: make it tweakable?
# }}} - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Third: parameters: these are the ones whose values we are looking for: # {{{ -
# probabilities of capture on each occasion
  P[T],
# probabilities of each capture event given that capture occured
  delta[nbCaptureEvents],
# last, but not least, the famous free elements of x: d, who lives in the
# convex polytope. They represent latent counts of unobservable histories.
  d[LB],
# }}} - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# Fourth: intermediate values, just technically helpful # {{{ - - - - - - - - -
# count values for unsure latent histories:
  x[LU + LB],
# "full x": count values for all latent histories:
  X[L + is.Sf.empty],
# Probability of being capture at least once
  G,
# Probabilities of capture, conditionnally that there is any capture at all
  p[T],
# convenience iterator for handling "no S-histories" case in the likelihood-loop
  it[L],
# log-probabilities of each capture event in Omega (for machine accuracy)
  ln.pi.t[L,T],
# log-probabilities for each capture history (for machine accuracy)
  ln.pi[L],
# log-factors of the likelihood: (for machine accuracy)
  ln.Likelihood.f[L],
# log-likelihood (for machine accuracy)
  ln.Likelihood,
# uniform values in [0,1] used for sampling in the bounding box
  unifs[LB],
# dummy node used for rejecting unpositive X values, given as data to 1
  BernoulliTrick,
# corresponding parameter: 0 if the X is to be rejected, 1 if not
  BernoulliTrickP,
# test whether all x's are positive:
  xPos[LU + LB],
  allxPos,
# latent number of captured individuals (sum(X))
  n,
# total estimated number of individuals
  N,
# high constant for the poisson trick:
  PoissonC,
# Probability for PoissonTrick:
  lambda,
# dummy for the poisson trick, given as data to 1
  PoissonTrick;
# }}} - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

# }}}

model {


  # Priors on capture parameters: # {{{ - - - - - - - - - - - - - - - - - - - -

  # The probabilities of capture have been assumed to come from a beta
  # distribution
  for (t in 1:T){
    P[t] ~ dbeta(h.P[1], h.P[2])
    p[t] <- P[t] / G
  }
  # And then they are observed:
  G <- 1 - prod(1 - P[])

  # The probabilities of capture events have been assumed to come from a
  # dirichlet distribution:
  delta[] ~ ddirch(h.delta[])

  # }}} - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


  # Latent counts X #{{{ -  - - - - - - - - - - - - - - - - - - - - - - - - - -

  # W need to trick JAGS in order to make it reject the negative X.
  # The node BernoulliTrick will be given as a data value, 1, and its
  # distribution will be certain, either 0 or 1 depending on the value X being
  # consistent or not.

  # Sample d from the bounding box:
  for(i in 1:LB){
    unifs[i] ~ dunif(0,1)
    d[i] <- trunc(unifs[i] * (box.ceilings[i] + 1))
  }

  # Rebuild x from d using the kernel B of the observation matrix A
  x <- B %*% d + x0
  # if any of x is negative, reject the proposal:
  for(i in 1:LM){
    xPos[i] <- step(x[i]) # check each x
  }
  allxPos <- sum(xPos) # sum all the tests
  BernoulliTrickP <- step((allxPos == LM) - 1) # set it to 0 if any failed..
  # so that this'll be unconsistent with the data then, and the sample will be..
  BernoulliTrick ~ dbern(BernoulliTrickP) # ..rejected.

  # Rebuild X: gather all latent histories counts together
  # 1) Sure stories (may be dummy data if the true `S.f` vector is empty)
  for(i in 1:jagsLS){ X[i] <- jagsS.f[i] }
  # 2) unobservable stories and ghosts
  for(i in 1:LM){ X[i + jagsLS] <- x[i] }

  # }}} - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


  # Likelihood # {{{ - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

  # Probabilities of each single capture event in Omega:
  # (conditional on capture)
  for(omega in 1:L){ # iterate on all latent stories
    for(t in 1:T){   # then on all capture occasions
      ln.pi.t[omega, t] <- # the log-probability of this event happening
          step((Omega[omega, t] == 0) - 1) * log(1-p[t]) # *no* capture or..  0
        + step((Omega[omega, t] != 0) - 1) * log(p[t])   # ..*capture*, then: -
        + step((Omega[omega, t] == 1) - 1) * log(delta[2]) # left encounter   L
        + step((Omega[omega, t] == 2) - 1) * log(delta[3]) # right encounter  R
        + step((Omega[omega, t] == 3) - 1) * log(delta[4]) # both encounters  B
        + step((Omega[omega, t] == 4) - 1) * log(delta[5]) # simultaneous     S
    }
    # log-Probability of a latent encounter history:
    ln.pi[omega] <- sum(ln.pi.t[omega,])

    # Correct the iterator if there is a empty S.f, not to take it into account
    it[omega] <- omega + is.Sf.empty

    # # Explicit each factor of the total log-likelihood:
    # ln.Likelihood.f[omega] <- X[it[omega]] * ln.pi[omega]
    #                         - logfact(X[it[omega]])
    # For some reason, logfact(0) prints an annoying warning "value out of range
    # in 'lgamma'", so we need to trick: (logfact(0) = 0)
    # I know this looks useless.. but this `step` function silents it down :
    ln.Likelihood.f[omega] <- X[it[omega]] * ln.pi[omega]
                            - logfact(step(X[it[omega]] - 1) * X[it[omega]])
  }

  # Poisson trick!
  # Likelihood, formula for the multinomial distribution:
  n <- sum(X) # this is why the fake `S.f` data, if they exist must be 0
  ln.Likelihood <- logfact(n) + sum(ln.Likelihood.f[])
                 - n * log(G) - (N - n) * log(1 - G)
  lambda <- PoissonC - ln.Likelihood
  PoissonTrick ~ dpois(lambda)

  # }}} - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


  # Conclude with our estimation:
  N <- n / G


}

